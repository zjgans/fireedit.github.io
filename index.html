<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>ConsistentID</title>
  <style>
    /* 居中和放大标题 */
    .abstract-title {
      text-align: center;
      /* 文本居中 */
      font-size: 10em;
      /* 设置字体大小 */
    }

    .author-block {
      position: relative;
    }

    .author-block a {
      position: relative;
      /* 让链接相对定位，以便数字在链接之上 */
    }



    .cart-count {
      position: absolute;
      top: -8px;
      right: -8px;
      background-color: rgb(159, 156, 156);
      color: white;
      padding: 2px 4px;
      font-size: 8px;
      border-radius: 50%;
    }
  </style>

  <link rel="icon" type="image/x-icon" href="static\images\titleLogo.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">FireEdit: Fine-grained Instruction-based Image Editing via Region-aware Vision Language Model</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a  target="_blank">Jun Zhou</a><span class="cart-count">1</span>
              </span>,
              <span class="author-block">
                <a target="_blank">Jiahao Li</a><span class="cart-count">2</span>
              </span>,
              <span class="author-block">
                <a target="_blank">Zunnan Xu</a><span class="cart-count">3</span>
              </span>,
              <span class="author-block">
                <a target="_blank">Hanhui Li</a><span class="cart-count">1</span>
              </span>,
              <span class="author-block">
                <a target="_blank">Yiji Chen</a><span class="cart-count">3</span>
              </span>,
              <span class="author-block">
                <a href="https://harlanhong.github.io/" target="_blank">Fa-ting Hong</a><span class="cart-count">4</span>
              </span>,
              <span class="author-block">
                <a target="_blank">Qin Lin</a><span class="cart-count">3</span>
              </span>,
              <span class="author-block">
                <a target="_blank">Qinglin Lu</a><span class="cart-count">3</span>
              </span>,
              <span class="author-block">
                <a href="https://www.sysu-hcp.net/faculty/xiaodanliang.html" arget="_blank">Xiaodan Liang</a><span
                  class="cart-count">1</span>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">1. Shenzhen Campus of Sun Yat-sen University,</span>
              <span class="author-block">2. Tencent Hunyuan,</span>
              <span class="author-block">3. Tsinghua University,</span>
              <span class="author-block">4. HKUST</span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="arXiv.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <span class="link-block">
                  <a  href="https://huggingface.co/spaces/JackAILab/ConsistentID/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-play-circle"></i>
                    </span>
                    <span>Demo</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/JackAILab/ConsistentID" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="http://arxiv.org/abs/2404.16771" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser image-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- Your image here -->
        <img src="static\images\teaser.jpg" alt="Teaser Image" width="100%" height="auto">
        <h2 class="subtitle has-text-centered">
           We propose FireEdit, an instruction-based image editing method guided by a visual language model (VLM). Our primary innovation is the introduction of region tokens,
  which enable the VLM to accurately identify edited objects or areas in complex scenarios while preserving high-frequency details in unintended regions during image decoding.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser image -->


  <!-- Teaser video-->
  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        Your video here
        <source src="\static\PDF\teaser.pdf" type="application/pdf">
        <h2 class="subtitle has-text-centered">
          We propose FireEdit, an instruction-based image editing method guided by a visual language model (VLM). Our primary innovation is the introduction of region tokens,
  which enable the VLM to accurately identify edited objects or areas in complex scenarios while preserving high-frequency details in unintended regions during image decoding.
        </h2>
      </div>
    </div>
  </section> -->
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Currently, instruction-based image editing methods have made significant progress by leveraging the powerful cross-modal understanding capabilities of visual language models (VLMs).
              However, they still face challenges in three key areas: 1) complex scenarios; 2) semantic consistency; and 3) fine-grained editing. 
              To address these issues, we propose FireEdit, an innovative Fine-grained Instruction-based image editing framework that exploits a REgion-aware VLM. 
FireEdit is designed to accurately comprehend user instructions and ensure effective control over the editing process. We employ a VLM to precisely localize the desired editing regions within complex scenes. 
              To enhance the fine-grained visual perception capabilities of the VLM, we introduce additional region tokens that complement the holistic image features and are integrated into the user's instructions. 
              Relying solely on the output of the Language Model (LLM) to guide the diffusion model may result in suboptimal editing outcomes.
Therefore, we propose a Time-Aware Target Injection module and a Hybrid Visual Cross Attention module. 
              The former dynamically adjusts the guidance strength at various denoising stages by integrating timestep embeddings with the text embeddings. The latter enhances visual details for image editing, thereby preserving semantic consistency between the edited result and the source image. By combining the VLM enhanced with fine-grained region tokens and the time-dependent diffusion model, FireEdit demonstrates significant advantages in comprehending editing instructions and maintaining high semantic consistency. Extensive experiments indicate that our approach surpasses the state-of-the-art instruction-based image editing methods.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Teaser image-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- Your image here -->
        <h2 class="title is-3 abstract-title">Facial feature details</h2>
        <img src="static\images\FacialCompare.jpg" alt="MY ALT TEXT" />
        <h2 class="subtitle has-text-centered">
          Comparison of facial feature details between our method and existing approaches.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser image -->

  <!-- Framework-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- Your image here -->
        <h2 class="title is-3 abstract-title">Framework</h2>
        <img src="static\images\Framework_page-0001.jpg" alt="Teaser Image" width="100%" height="auto">
        <h2 class="subtitle has-text-centered">
          The overall framework of our proposed ConsistentID.
        </h2>
        <h2>
          The framework comprises two key modules: a multimodal facial ID generator and a purposefully crafted
          ID-preservation
          network. The multimodal facial prompt generator consists of two essential components: a fine-grained
          multimodal feature
          extractor, which focuses on capturing detailed facial information, and a facial ID feature extractor dedicated
          to
          learning facial ID features. On the other hand, the ID-preservation network utilizes both facial textual and
          visual
          prompts, preventing the blending of ID information from different facial regions through the facial attention
          localization strategy. This approach ensures the preservation of ID consistency in the facial regions.
        </h2>
      </div>
    </div>
  </section>
  <!-- End Framework -->


  <!-- Image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <h2 class="title is-3 abstract-title">Application</h2>

      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <!-- Your image here -->
            <img src="static\images\Application_page-0001.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              The comparisons of two downstream applications.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static\images\mix_page-0001.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Application cases of ConsistentID for identity confusion.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static\images\old_page-0001.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Application cases of ConsistentID for bringing old photos back to life.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static\images\age_page-0001.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Application cases of ConsistentID for altering the age attribute of a character.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static\images\sfhq_page-0001.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Application on SFHQ test set.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End image carousel -->






  <!-- Image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <h2 class="title is-3 abstract-title">Comparation</h2>
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <!-- Your image here -->
            <img src="static\images\ResultsCompare_page-0001.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Qualitative comparison of universal recontextualization samples is conducted, comparing our approach with
              other methods using five distinct identities and their corresponding prompts. Our ConsistentID exhibits a
              more powerful capability in high-quality generation, flexible editability, and strong identity fidelity.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static\images\Photomaker_InstantID_Compare_page-0001.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Qualitative comparison of our model with other models on two special tasks: stylization and action
              instruction.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static\images\compare_page-0001.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              The comparisons with more fine-tuning-based models.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static\images\IPACompare_page-0001.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Comparison of ConsistentID with IP-Adapter and its face version variants conditioned on different styles.
            </h2>
          </div>
        </div>
        <div class="item">
          <!-- Your image here -->
          <img src="static\images\All_Model_Vis_page-0001.jpg" alt="MY ALT TEXT" />
          <h2 class="subtitle has-text-centered">
            Visualization in re-contextualization settings. These examples demonstrate the high-identity fidelity and
            text editing
            capability of ConsistentID.
          </h2>
        </div>
      </div>
    </div>
  </section>
  <!-- End image carousel -->

  <!-- Image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <h2 class="title is-3 abstract-title">Ablation experiment</h2>
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <!-- Your image here -->
            <img src="static\images\Vis_AttentionLoss_page-0001.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Visualized results with or without using attention loss.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static\images\Vis_DelayControl_page-0001.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Visualized results under different `merge steps'. `Merge Step' indicates when to start adding facial image
              features to the text prompt.
            </h2>
          </div>
        </div>
      </div>
  </section>
  <!-- End image carousel -->


  <!-- Paper poster -->
  <!-- <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster</h2>

        <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
        </iframe>

      </div>
    </div>
  </section> -->
  <!--End paper poster -->


  <!-- BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
  </section> -->
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
